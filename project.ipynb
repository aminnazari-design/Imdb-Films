{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver \n",
    "# from selenium.webdriver.common.by import By\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting datas from Imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Chronicles the experiences of a formerly succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>The Godfather \"Don\" Vito Corleone is the head ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Set within a year after the events of Batman B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>The continuing saga of the Corleone crime fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>The defense and the prosecution have rested, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  \\\n",
       "0   The Shawshank Redemption   \n",
       "1              The Godfather   \n",
       "2            The Dark Knight   \n",
       "3      The Godfather Part II   \n",
       "4               12 Angry Men   \n",
       "\n",
       "                                                Plot  \n",
       "0  Chronicles the experiences of a formerly succe...  \n",
       "1  The Godfather \"Don\" Vito Corleone is the head ...  \n",
       "2  Set within a year after the events of Batman B...  \n",
       "3  The continuing saga of the Corleone crime fami...  \n",
       "4  The defense and the prosecution have rested, a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url =  \"https://www.imdb.com/chart/top/\"\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\"\n",
    "headers = {\"user-agent\" : user_agent}\n",
    "# driver  = webdriver.Chrome()\n",
    "source = requests.get(url , headers=headers)\n",
    "# print(source.status_code)\n",
    "# driver.find_element(By.ID , \"list-view-option-detailed\").click()\n",
    "time.sleep(2)\n",
    "# print(driver.find_element(By.XaPATH , \"html/body/div[1]\").find_element(By.CLASS_NAME , \"ipc-page-wrapper\").get_attribute(\"innerHTML\"))\n",
    "source = BeautifulSoup(source.content , \"html.parser\")\n",
    "# print(source)\n",
    "# data = []\n",
    "names_and_links = list()\n",
    "df = pd.DataFrame(columns=[\"Name\" , \"Plot\"])\n",
    "for item in source.find(\"ul\" , class_=\"ipc-metadata-list\").find_all(\"li\"):\n",
    "    # print(item.find(\"a\")[\"href\"])\n",
    "    name = item.find(\"h3\" , class_=\"ipc-title__text\").text\n",
    "    print()\n",
    "    names_and_links.append((name , 'https://www.imdb.com' + \"/\".join(item.find(\"a\")[\"href\"].split(\"/\")[0:3] + [\"plotsummary/?ref_=tt_stry_pl\"])))\n",
    "# print(names_and_links[0])\n",
    "\n",
    "# print((list(source.findAll(\"div\" , class_ = \"ipc-html-content-inner-div\")))[1])\n",
    "# print(list(map(lambda x: x.text , list(source.findAll(\"div\" , class_ = \"ipc-page-section ipc-page-section--base celwidget\")))))\n",
    "    i = 0\n",
    "for x,y in names_and_links:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    res = requests.get(y , headers=headers)\n",
    "    # time.sleep(3)\n",
    "\n",
    "    source = BeautifulSoup(res.content , \"html.parser\")\n",
    "    ul = (source.find(\"ul\" , class_ = \"ipc-metadata-list\"))\n",
    "                                    #   \"ipc-metadata-list ipc-metadata-list--dividers-after sc-d1777989-0 FVBoi ipc-metadata-list--base\"\n",
    "                                    #   \"ipc-metadata-list ipc-metadata-list--dividers-after sc-d1777989-0 FVBoi ipc-metadata-list--base\"\n",
    "                                    #   \"ipc-metadata-list ipc-metadata-list--dividers-between sc-d1777989-0 FVBoi meta-data-list-full ipc-metadata-list--base\"\n",
    "    # print(ul)\n",
    "    # print(list(ul.find_all(\"li\"))[1].text)\n",
    "    plot = ul.find_all(\"li\")[1].text\n",
    "#     print(y)\n",
    "#     res = requests.get(y , headers=headers)\n",
    "#     source = BeautifulSoup(source.content , \"html.parser\")\n",
    "#     print(source)\n",
    "    # plot = item.find(\"div\"  , class_ = \"ipc-html-content-inner-div\").text\n",
    "    dict = { 'Name' :x.split(\".\")[1], 'Plot':plot }\n",
    "    new_row = pd.DataFrame(dict , index=[x.split(\".\")[0]])\n",
    "    df = pd.concat([df , new_row] , ignore_index=True)\n",
    "df.to_csv(\"datas.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = None\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    res = f.readlines()\n",
    "res = list(map(lambda x: x.replace(\"\\n\" , \"\") , res ))\n",
    "res = list(filter(lambda x : len(x)>1 , res))\n",
    "stop_words = res\n",
    "# print(res)\n",
    "# print(list(filter(lambda x : len(x)<2 , stop_words)) ,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>Plot</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Chronicles the experiences of a formerly succe...</td>\n",
       "      <td>chronicles experiences formerly successful ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>The Godfather \"Don\" Vito Corleone is the head ...</td>\n",
       "      <td>godfather \"don\" vito corleone head corleone ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Set within a year after the events of Batman B...</td>\n",
       "      <td>set events batman begins (2005) batman lieuten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>The continuing saga of the Corleone crime fami...</td>\n",
       "      <td>continuing saga corleone crime family tells st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>The defense and the prosecution have rested, a...</td>\n",
       "      <td>defense prosecution rested jury filing jury de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       Name  \\\n",
       "0           0   The Shawshank Redemption   \n",
       "1           1              The Godfather   \n",
       "2           2            The Dark Knight   \n",
       "3           3      The Godfather Part II   \n",
       "4           4               12 Angry Men   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Chronicles the experiences of a formerly succe...   \n",
       "1  The Godfather \"Don\" Vito Corleone is the head ...   \n",
       "2  Set within a year after the events of Batman B...   \n",
       "3  The continuing saga of the Corleone crime fami...   \n",
       "4  The defense and the prosecution have rested, a...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  chronicles experiences formerly successful ban...  \n",
       "1  godfather \"don\" vito corleone head corleone ma...  \n",
       "2  set events batman begins (2005) batman lieuten...  \n",
       "3  continuing saga corleone crime family tells st...  \n",
       "4  defense prosecution rested jury filing jury de...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas = pd.read_csv(\"datas.csv\")\n",
    "\n",
    "new_res = list()\n",
    "for item in datas[\"Plot\"]:\n",
    "    item = item.replace(\".\" , \"\").replace(\"!\" , \"\").replace(\"?\" , \"\").replace(\",\" , \"\").replace(\"'\" , '').replace(\"'\" , \"\")\n",
    "    x= item.split()\n",
    "    \n",
    "    x = list(map(lambda v: v.lower() , x ))\n",
    "    # print(x)\n",
    "    # for u in x:\n",
    "    #     print(u)\n",
    "    #     if u in res  + [\"a\"]:\n",
    "    #         # print(\"it did\")\n",
    "    #         x.remove(u)\n",
    "    # print(x)\n",
    "    x = list(filter(lambda x: x  not in res+[\"a\"] , x))\n",
    "    # print(x)\n",
    "    item = \" \".join(x)\n",
    "    new_res.append(item)\n",
    "    # item = item.lower()\n",
    "datas[\"cleaned\"] = new_res \n",
    "datas.head()\n",
    "# datas[\"Plot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words in plots for creating dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life',\n",
       " 'time',\n",
       " 'world',\n",
       " '-',\n",
       " 'family',\n",
       " 'story',\n",
       " 'war',\n",
       " 'love',\n",
       " 'named',\n",
       " 'own',\n",
       " 'wife',\n",
       " 'police',\n",
       " 'father',\n",
       " 'son',\n",
       " 'soon',\n",
       " 'lives',\n",
       " 'day',\n",
       " 'boy',\n",
       " 'job',\n",
       " 'woman',\n",
       " 'help',\n",
       " 'film',\n",
       " 'people',\n",
       " 'called',\n",
       " 'including',\n",
       " 'friend',\n",
       " 'town',\n",
       " 'friends',\n",
       " 'john',\n",
       " 'despite',\n",
       " 'begins',\n",
       " 'home',\n",
       " 'takes',\n",
       " 'discovers',\n",
       " 'sent',\n",
       " 'set',\n",
       " 'fight',\n",
       " 'living',\n",
       " 'children',\n",
       " 'able',\n",
       " 'girl',\n",
       " 'plan',\n",
       " 'comes',\n",
       " 'live',\n",
       " 'death',\n",
       " 'city',\n",
       " 'american',\n",
       " 'hes',\n",
       " 'killed',\n",
       " 'daughter',\n",
       " 'makes',\n",
       " 'army',\n",
       " 'pictures',\n",
       " 'learns',\n",
       " 'decides',\n",
       " 'child',\n",
       " 'former',\n",
       " 'eventually',\n",
       " 'tries',\n",
       " 'care',\n",
       " 'crime',\n",
       " 'mysterious',\n",
       " 'tells',\n",
       " 'leads',\n",
       " 'meet',\n",
       " 'meets',\n",
       " 'meanwhile',\n",
       " 'company',\n",
       " 'jack',\n",
       " 'mother',\n",
       " 'park',\n",
       " 'house',\n",
       " 'found',\n",
       " 'murder',\n",
       " 'money',\n",
       " 'kill',\n",
       " 'battle',\n",
       " 'escape',\n",
       " 'power',\n",
       " 'falls',\n",
       " 'eve',\n",
       " 'earth',\n",
       " 'trying',\n",
       " 'parents',\n",
       " 'school',\n",
       " 'wealthy',\n",
       " 'following',\n",
       " 'husband',\n",
       " 'crew',\n",
       " 'believes',\n",
       " 'riganas',\n",
       " 'powerful',\n",
       " 'future',\n",
       " 'ring',\n",
       " 'lost',\n",
       " 'little',\n",
       " 'themselves',\n",
       " 'samurai',\n",
       " 'captain',\n",
       " 'join',\n",
       " 'black',\n",
       " 'real',\n",
       " 'true',\n",
       " 'control',\n",
       " 'quest',\n",
       " 'night',\n",
       " 'de',\n",
       " 'past',\n",
       " 'killer',\n",
       " 'learn',\n",
       " 'officer',\n",
       " 'returns',\n",
       " 'arrives',\n",
       " 'village',\n",
       " 'try',\n",
       " 'play',\n",
       " 'action',\n",
       " 'camp',\n",
       " 'forces',\n",
       " 'evil',\n",
       " 'chance',\n",
       " 'ultimately',\n",
       " 'discover',\n",
       " 'deadly',\n",
       " 'mind',\n",
       " 'team',\n",
       " 'journey',\n",
       " 'bring',\n",
       " 'prison',\n",
       " '--',\n",
       " 'dead',\n",
       " 'question',\n",
       " 'dr',\n",
       " 'return',\n",
       " 'brother',\n",
       " 'protect',\n",
       " 'cant',\n",
       " 'hotel',\n",
       " 'maria',\n",
       " 'based',\n",
       " 'destroy',\n",
       " 'days',\n",
       " 'strange',\n",
       " 'dark',\n",
       " 'childhood',\n",
       " 'vietnam',\n",
       " 'encounter',\n",
       " 'enemy',\n",
       " 'princess',\n",
       " 'force',\n",
       " 'movie',\n",
       " 'realizes',\n",
       " 'finally',\n",
       " 'white',\n",
       " 'forced',\n",
       " 'revenge',\n",
       " 'station',\n",
       " 'plans',\n",
       " 'professional',\n",
       " 'investigation',\n",
       " 'fathers',\n",
       " 'goes',\n",
       " 'society',\n",
       " 'hospital',\n",
       " 'colonel',\n",
       " '&',\n",
       " 'doesnt',\n",
       " 'space',\n",
       " 'professor',\n",
       " 'sets',\n",
       " 'british',\n",
       " 'accident',\n",
       " 'tony',\n",
       " 'island',\n",
       " 'merrick',\n",
       " 'head',\n",
       " 'york',\n",
       " 'business',\n",
       " 'follows',\n",
       " '\"the',\n",
       " 'personal',\n",
       " 'stop',\n",
       " 'decide',\n",
       " 'german',\n",
       " 'fighting',\n",
       " 'boxer',\n",
       " 'gold',\n",
       " 'leaves',\n",
       " 'dangerous',\n",
       " 'instead',\n",
       " 'entire',\n",
       " 'human',\n",
       " 'result',\n",
       " 'career',\n",
       " 'talking',\n",
       " 'key',\n",
       " 'earlier',\n",
       " 'hold',\n",
       " 'leader',\n",
       " 'apartment',\n",
       " 'class',\n",
       " 'dreams',\n",
       " 'university',\n",
       " 'looking',\n",
       " 'quickly',\n",
       " 'run',\n",
       " 'meeting',\n",
       " 'london',\n",
       " 'japanese',\n",
       " 'wishes',\n",
       " 'unknown',\n",
       " 'french',\n",
       " 'country',\n",
       " 'don',\n",
       " 'unexpected',\n",
       " 'train',\n",
       " 'dog',\n",
       " 'mans',\n",
       " 'michael',\n",
       " 'refuses',\n",
       " 'mob',\n",
       " 'events',\n",
       " 'begin',\n",
       " 'criminal',\n",
       " 'detective',\n",
       " 'boss',\n",
       " 'wallace',\n",
       " 'bill',\n",
       " 'hired',\n",
       " 'union',\n",
       " 'finding',\n",
       " 'famous',\n",
       " 'starts',\n",
       " 'prove',\n",
       " 'move',\n",
       " 'luke',\n",
       " 'base',\n",
       " 'actions',\n",
       " 'computer',\n",
       " 'tommy',\n",
       " 'band',\n",
       " 'hunt',\n",
       " 'green',\n",
       " 'hard',\n",
       " 'chief',\n",
       " 'getting',\n",
       " 'jewish',\n",
       " 'teacher',\n",
       " 'leave',\n",
       " 'mission',\n",
       " 'rescue',\n",
       " 'beautiful',\n",
       " 'ki-woo',\n",
       " 'tour',\n",
       " 'frank',\n",
       " 'knowing',\n",
       " 'close',\n",
       " 'news',\n",
       " 'brings',\n",
       " 'murdered',\n",
       " 'creature',\n",
       " 'girls',\n",
       " 'stay',\n",
       " 'provide',\n",
       " 'becoming',\n",
       " 'murderer',\n",
       " 'wall-e',\n",
       " 'jones',\n",
       " 'hero',\n",
       " 'led',\n",
       " 'street',\n",
       " 'especially',\n",
       " 'soldiers',\n",
       " 'cross',\n",
       " 'angela',\n",
       " 'woody',\n",
       " 'party',\n",
       " 'century',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'giovanna',\n",
       " 'prisoners',\n",
       " 'corleone',\n",
       " 'rival',\n",
       " 'doing',\n",
       " 'criminals',\n",
       " 'struggle',\n",
       " 'nazi',\n",
       " 'feels',\n",
       " 'save',\n",
       " 'reach',\n",
       " 'name',\n",
       " 'reason',\n",
       " 'king',\n",
       " 'dream',\n",
       " 'loved',\n",
       " 'perfect',\n",
       " 'rebel',\n",
       " 'empire',\n",
       " 'moves',\n",
       " 'message',\n",
       " 'reality',\n",
       " 'beyond',\n",
       " 'government',\n",
       " 'robbery',\n",
       " 'start',\n",
       " 'success',\n",
       " 'trouble',\n",
       " 'desperate',\n",
       " 'seven',\n",
       " 'bright',\n",
       " 'inside',\n",
       " 'discovered',\n",
       " 'gone',\n",
       " 'taking',\n",
       " 'attack',\n",
       " 'figure',\n",
       " 'game',\n",
       " 'search',\n",
       " 'happens',\n",
       " 'brothers',\n",
       " 'united',\n",
       " 'god',\n",
       " 'remaining',\n",
       " 'tale',\n",
       " 'universe',\n",
       " 'tried',\n",
       " 'justice',\n",
       " 'secret',\n",
       " 'gradually',\n",
       " 'ii',\n",
       " 'student',\n",
       " 'office',\n",
       " 'married',\n",
       " 'heads',\n",
       " 'towards',\n",
       " 'loss',\n",
       " 'dies',\n",
       " 'rest',\n",
       " 'position',\n",
       " 'drug',\n",
       " 'manages',\n",
       " 'killing',\n",
       " 'break',\n",
       " 'violent',\n",
       " 'gang',\n",
       " 'music',\n",
       " 'wrong',\n",
       " 'due',\n",
       " 'pay',\n",
       " 'local',\n",
       " 'america',\n",
       " 'paris',\n",
       " 'native',\n",
       " 'spends',\n",
       " 'deal',\n",
       " 'except',\n",
       " 'ship',\n",
       " 'moving',\n",
       " 'assistance',\n",
       " 'blood',\n",
       " 'max',\n",
       " 'leading',\n",
       " 'public',\n",
       " 'isnt',\n",
       " 'baby',\n",
       " 'department',\n",
       " 'lawyer',\n",
       " 'missing',\n",
       " 'ishaan',\n",
       " 'road',\n",
       " 'mark',\n",
       " 'pregnant',\n",
       " 'mulwray',\n",
       " 'torture',\n",
       " 'car',\n",
       " 'jesse',\n",
       " 'hirayama',\n",
       " 'vito',\n",
       " 'wedding',\n",
       " 'respect',\n",
       " 'ruthless',\n",
       " 'growing',\n",
       " 'attempts',\n",
       " 'trial',\n",
       " 'unlikely',\n",
       " 'jews',\n",
       " 'frodo',\n",
       " 'sam',\n",
       " 'fate',\n",
       " 'facing',\n",
       " 'danger',\n",
       " 'civil',\n",
       " 'alive',\n",
       " 'eyes',\n",
       " 'look',\n",
       " 'attempt',\n",
       " 'art',\n",
       " 'idea',\n",
       " 'seen',\n",
       " 'legendary',\n",
       " 'hidden',\n",
       " 'seek',\n",
       " 'path',\n",
       " 'neo',\n",
       " 'henry',\n",
       " 'involved',\n",
       " 'crimes',\n",
       " 'remains',\n",
       " 'victim',\n",
       " 'understand',\n",
       " 'efforts',\n",
       " 'uncle',\n",
       " 'billy',\n",
       " 'bank',\n",
       " 'loves',\n",
       " 'suicide',\n",
       " 'ones',\n",
       " 'request',\n",
       " 'west',\n",
       " 'media',\n",
       " 'women',\n",
       " 'water',\n",
       " 'martin',\n",
       " 'brazil',\n",
       " 'violence',\n",
       " 'fear',\n",
       " 'population',\n",
       " 'witnesses',\n",
       " 'paul',\n",
       " 'terminator',\n",
       " 'cruel',\n",
       " 'seeks',\n",
       " 'various',\n",
       " 'kims',\n",
       " 'half',\n",
       " 'parks',\n",
       " 'housekeeper',\n",
       " 'dollars',\n",
       " 'california',\n",
       " 'unable',\n",
       " 'top',\n",
       " 'prince',\n",
       " 'rules',\n",
       " 'south',\n",
       " 'race',\n",
       " 'identity',\n",
       " 'corrupt',\n",
       " 'agent',\n",
       " 'danny',\n",
       " 'andrew',\n",
       " 'goal',\n",
       " 'happy',\n",
       " 'playing',\n",
       " 'clan',\n",
       " 'runs',\n",
       " 'tough',\n",
       " 'decision',\n",
       " 'boys',\n",
       " 'whom',\n",
       " 'distant',\n",
       " 'friendship',\n",
       " 'sold',\n",
       " 'partner',\n",
       " 'stories',\n",
       " 'process',\n",
       " 'land',\n",
       " 'aid',\n",
       " 'corporation',\n",
       " 'humans',\n",
       " 'brody',\n",
       " 'existence',\n",
       " 'joe',\n",
       " 'six',\n",
       " 'moment',\n",
       " 'memory',\n",
       " 'alien',\n",
       " 'marines',\n",
       " 'knight',\n",
       " 'soviet',\n",
       " 'estate',\n",
       " 'oh',\n",
       " 'television',\n",
       " 'mozart',\n",
       " 'joseph',\n",
       " 'andy',\n",
       " 'struggles',\n",
       " 'campaign',\n",
       " 'forest',\n",
       " 'shes',\n",
       " 'sister',\n",
       " 'luck',\n",
       " 'couple',\n",
       " 'streets',\n",
       " 'sara',\n",
       " 'watching',\n",
       " 'current',\n",
       " 'college',\n",
       " 'insurance',\n",
       " 'suspects',\n",
       " 'judy',\n",
       " 'amélie',\n",
       " 'cops',\n",
       " 'joy',\n",
       " 'characters',\n",
       " 'cousin',\n",
       " 'driver',\n",
       " 'network',\n",
       " 'daniel',\n",
       " 'nicky',\n",
       " 'sons',\n",
       " 'award',\n",
       " 'nemo',\n",
       " 'sea',\n",
       " 'priest',\n",
       " 'gittes',\n",
       " 'engineer',\n",
       " 'supposed',\n",
       " 'issue',\n",
       " 'benjamín',\n",
       " 'behavior',\n",
       " 'conflict',\n",
       " 'kowalski',\n",
       " 'replicants',\n",
       " 'celine',\n",
       " 'vienna',\n",
       " 'spend',\n",
       " 'book',\n",
       " 'rajkannu',\n",
       " 'guilty',\n",
       " 'dealing',\n",
       " 'event',\n",
       " 'daughters',\n",
       " 'families',\n",
       " 'lieutenant',\n",
       " 'james',\n",
       " 'successfully',\n",
       " 'chaos',\n",
       " 'bruce',\n",
       " 'saga',\n",
       " 'innocent',\n",
       " 'accused',\n",
       " 'refuge',\n",
       " 'concentration',\n",
       " 'vincent',\n",
       " 'stolen',\n",
       " 'employer',\n",
       " 'aging',\n",
       " 'major',\n",
       " 'writes',\n",
       " 'times',\n",
       " 'emotional',\n",
       " 'cobb',\n",
       " 'stealing',\n",
       " 'deep',\n",
       " 'corporate',\n",
       " 'task',\n",
       " 'plant',\n",
       " 'alliance',\n",
       " 'skywalker',\n",
       " 'han',\n",
       " 'solo',\n",
       " 'trials',\n",
       " 'friendships',\n",
       " 'navigate',\n",
       " 'lead',\n",
       " 'consequences',\n",
       " 'truth',\n",
       " 'machines',\n",
       " 'confront',\n",
       " 'else',\n",
       " 'mcmurphy',\n",
       " 'court',\n",
       " 'worlds',\n",
       " 'effort',\n",
       " 'ensure',\n",
       " 'travel',\n",
       " 'system',\n",
       " 'george',\n",
       " 'spent',\n",
       " 'giving',\n",
       " 'opportunity',\n",
       " 'prevent',\n",
       " 'rich',\n",
       " 'christmas',\n",
       " 'loses',\n",
       " 'jail',\n",
       " 'thinking',\n",
       " 'mary',\n",
       " 'coming',\n",
       " 'promise',\n",
       " 'giant',\n",
       " 'aspires',\n",
       " 'lecter',\n",
       " 'victims',\n",
       " 'assignment',\n",
       " 'located',\n",
       " 'allied',\n",
       " 'receive',\n",
       " 'staff',\n",
       " 'private',\n",
       " 'towns',\n",
       " 'novel',\n",
       " 'tom',\n",
       " 'choice',\n",
       " 'machine',\n",
       " 'sarah',\n",
       " 'connor',\n",
       " 'model',\n",
       " 'freedom',\n",
       " 'trip',\n",
       " 'adventures',\n",
       " 'encounters',\n",
       " 'extraordinary',\n",
       " 'survival',\n",
       " 'warsaw',\n",
       " 'change',\n",
       " 'poor',\n",
       " 'leaving',\n",
       " 'lived',\n",
       " 'using',\n",
       " 'worker',\n",
       " 'marion',\n",
       " 'seeing',\n",
       " 'drive',\n",
       " 'caught',\n",
       " 'main',\n",
       " 'needham',\n",
       " '<col@imdbcom>',\n",
       " 'games',\n",
       " 'rise',\n",
       " 'miles',\n",
       " 'adventure',\n",
       " 'information',\n",
       " 'suddenly',\n",
       " 'willing',\n",
       " 'golden',\n",
       " 'truck',\n",
       " 'contact',\n",
       " 'determined',\n",
       " 'jazz',\n",
       " 'musical',\n",
       " 'writer',\n",
       " 'means',\n",
       " 'fletcher',\n",
       " 'initially',\n",
       " 'believe',\n",
       " 'written',\n",
       " 'obsessed',\n",
       " 'shown',\n",
       " 'completely',\n",
       " 'perez',\n",
       " 'questioning',\n",
       " 'pressure',\n",
       " 'letters',\n",
       " 'told',\n",
       " 'relationship',\n",
       " 'hours',\n",
       " 'changes',\n",
       " 'silent',\n",
       " 'sound',\n",
       " 'line',\n",
       " 'mental',\n",
       " 'mistaken',\n",
       " 'follow',\n",
       " 'neighbors',\n",
       " 'suspect',\n",
       " 'enlists',\n",
       " 'girlfriend',\n",
       " 'spaceship',\n",
       " 'colony',\n",
       " 'schultz',\n",
       " 'owner',\n",
       " 'willard',\n",
       " 'walter',\n",
       " 'navy',\n",
       " 'bridge',\n",
       " 'build',\n",
       " 'memories',\n",
       " 'manoj',\n",
       " 'exams',\n",
       " 'ride',\n",
       " 'idol',\n",
       " 'abandoned',\n",
       " 'buy',\n",
       " 'left',\n",
       " 'history',\n",
       " 'enters',\n",
       " 'happened',\n",
       " 'outside',\n",
       " 'tv',\n",
       " 'barely',\n",
       " 'escapes',\n",
       " 'wiesler',\n",
       " 'east',\n",
       " 'loyal',\n",
       " 'wwi',\n",
       " 'ailing',\n",
       " 'employee',\n",
       " 'situation',\n",
       " 'barber',\n",
       " 'knowledge',\n",
       " 'sinister',\n",
       " 'escaping',\n",
       " 'theater',\n",
       " 'ripper',\n",
       " 'lester',\n",
       " 'infatuation',\n",
       " 'military',\n",
       " 'players',\n",
       " 'dae-su',\n",
       " 'abducted',\n",
       " 'brutal',\n",
       " 'antonio',\n",
       " 'toys',\n",
       " 'andys',\n",
       " 'buzz',\n",
       " 'engaged',\n",
       " 'attempting',\n",
       " 'william',\n",
       " 'free',\n",
       " 'robert',\n",
       " 'tokyo',\n",
       " 'safe',\n",
       " 'rancho',\n",
       " 'lina',\n",
       " 'sense',\n",
       " 'form',\n",
       " 'picture',\n",
       " 'kathy',\n",
       " 'initial',\n",
       " 'zain',\n",
       " 'retired',\n",
       " 'particular',\n",
       " 'harry',\n",
       " 'teddy',\n",
       " 'joel',\n",
       " 'lie',\n",
       " 'innocence',\n",
       " 'monolith',\n",
       " 'strangers',\n",
       " 'wont',\n",
       " 'passion',\n",
       " 'central',\n",
       " 'upside',\n",
       " 'reporters',\n",
       " 'placed',\n",
       " 'accept',\n",
       " 'carvalho',\n",
       " 'rio',\n",
       " 'janeiro',\n",
       " 'retirement',\n",
       " 'madeleine',\n",
       " 'incident',\n",
       " 'heart',\n",
       " 'score',\n",
       " 'onto',\n",
       " 'ago',\n",
       " 'hits',\n",
       " 'hooker',\n",
       " 'master',\n",
       " 'admits',\n",
       " 'powers',\n",
       " 'visit',\n",
       " 'unhappy',\n",
       " 'freder',\n",
       " 'workers',\n",
       " 'created',\n",
       " 'won',\n",
       " 'pair',\n",
       " 'cold',\n",
       " 'forever',\n",
       " 'waiting',\n",
       " 'eli',\n",
       " 'farm',\n",
       " 'oil',\n",
       " 'hammond',\n",
       " 'ellie',\n",
       " 'crowe',\n",
       " 'sheriff',\n",
       " 'logan',\n",
       " 'discovery',\n",
       " 'bride',\n",
       " 'arrival',\n",
       " 'england',\n",
       " 'ventures',\n",
       " 'woodcutter',\n",
       " 'tell',\n",
       " 'treves',\n",
       " 'bytes',\n",
       " 'mulwrays',\n",
       " 'fortune',\n",
       " 'scarlett',\n",
       " 'marry',\n",
       " 'pablo',\n",
       " 'emotions',\n",
       " 'magical',\n",
       " 'peter',\n",
       " 'doctor',\n",
       " 'summer',\n",
       " 'morning',\n",
       " 'tyrell',\n",
       " 'deckard',\n",
       " 'governor',\n",
       " 'barry',\n",
       " 'bombing',\n",
       " 'dubbed',\n",
       " 'shark',\n",
       " 'rat',\n",
       " 'hunting',\n",
       " 'monster',\n",
       " 'remote',\n",
       " 'smith',\n",
       " 'racing',\n",
       " 'reluctantly',\n",
       " 'tutsi',\n",
       " 'chris',\n",
       " 'sengeni',\n",
       " 'parker',\n",
       " 'sobinski',\n",
       " 'vijay',\n",
       " 'successful',\n",
       " 'commit',\n",
       " 'fellow',\n",
       " 'marine',\n",
       " 'treacherous',\n",
       " 'sell',\n",
       " 'drugs',\n",
       " 'dons',\n",
       " 'influence',\n",
       " 'clash',\n",
       " 'cause',\n",
       " 'batman',\n",
       " 'district',\n",
       " 'gotham',\n",
       " 'deeply',\n",
       " 'improve',\n",
       " '1950s',\n",
       " 'las',\n",
       " 'hollywood',\n",
       " 'jury',\n",
       " 'doubt',\n",
       " 'stage',\n",
       " 'managed',\n",
       " 'aragorn',\n",
       " 'retrieve',\n",
       " 'lose',\n",
       " 'series',\n",
       " 'lord',\n",
       " 'hope',\n",
       " 'buried',\n",
       " 'dying',\n",
       " 'forrest',\n",
       " 'teaches',\n",
       " 'creates',\n",
       " 'president',\n",
       " 'support',\n",
       " 'underground',\n",
       " 'scheme',\n",
       " 'spiral',\n",
       " 'rivalry',\n",
       " 'secrets',\n",
       " 'ability',\n",
       " 'impossible',\n",
       " 'heist',\n",
       " 'steal',\n",
       " 'amount',\n",
       " 'bros',\n",
       " 'continues',\n",
       " 'challenges',\n",
       " 'galactic',\n",
       " 'leia',\n",
       " 'courage',\n",
       " 'planet',\n",
       " 'receives',\n",
       " 'source',\n",
       " 'understanding',\n",
       " 'connection',\n",
       " 'presence',\n",
       " 'darth',\n",
       " 'vader',\n",
       " 'rebels',\n",
       " 'imagination',\n",
       " 'devoted',\n",
       " 'stopping',\n",
       " 'gangster',\n",
       " 'gangsters',\n",
       " 'affected',\n",
       " 'tinto',\n",
       " '<cst@imdbcom>',\n",
       " 'sentenced',\n",
       " 'stands',\n",
       " 'witness',\n",
       " 'abuse',\n",
       " 'rebellious',\n",
       " 'detectives',\n",
       " 'serial',\n",
       " 'ignorance',\n",
       " 'sin',\n",
       " 'newly',\n",
       " 'reaches',\n",
       " 'allows',\n",
       " 'potter',\n",
       " 'prevents',\n",
       " 'responsible',\n",
       " 'veteran',\n",
       " 'bandits',\n",
       " '6',\n",
       " 'supply',\n",
       " 'hide',\n",
       " 'unit',\n",
       " 'crawford',\n",
       " '(sir',\n",
       " 'anthony',\n",
       " 'solve',\n",
       " 'five',\n",
       " 'skin',\n",
       " 'response',\n",
       " 'piece',\n",
       " 'evidence',\n",
       " 'meaning',\n",
       " 'profile',\n",
       " 'catherine',\n",
       " 'senator',\n",
       " 'baker)',\n",
       " 'june',\n",
       " 'miller',\n",
       " 'send',\n",
       " 'gain',\n",
       " 'welcome',\n",
       " '1930s',\n",
       " 'guido',\n",
       " 'dora',\n",
       " 'horrors',\n",
       " 'prize',\n",
       " 'winning',\n",
       " 'hughes',\n",
       " 'stars',\n",
       " 'brought',\n",
       " 'destroyed',\n",
       " 'passed',\n",
       " 'failed',\n",
       " 'duo',\n",
       " 'typical',\n",
       " 'teenager',\n",
       " 'accidentally',\n",
       " 'mad',\n",
       " 'scientist',\n",
       " 'creatures',\n",
       " 'szpilman',\n",
       " 'pianist',\n",
       " 'operation',\n",
       " 'adult',\n",
       " 'ki-jung',\n",
       " 'lower',\n",
       " 'commercial',\n",
       " 'seoul',\n",
       " 'education',\n",
       " 'min',\n",
       " 'scam',\n",
       " 'names',\n",
       " 'difficult',\n",
       " 'rid',\n",
       " 'lover',\n",
       " 'breaks',\n",
       " 'storm',\n",
       " 'maximus',\n",
       " 'emperor',\n",
       " 'chooses',\n",
       " 'terry',\n",
       " 'morales',\n",
       " 'friendly',\n",
       " 'neighborhood',\n",
       " 'spider-man',\n",
       " 'grows',\n",
       " 'outcast',\n",
       " 'waging',\n",
       " 'cop',\n",
       " 'infiltrate',\n",
       " '12-year-old',\n",
       " 'dealer',\n",
       " 'methods',\n",
       " 'derek',\n",
       " 'committed',\n",
       " 'throughout',\n",
       " 'la',\n",
       " 'fresh',\n",
       " 'buddy',\n",
       " 'positive',\n",
       " 'sacrifice',\n",
       " 'audience',\n",
       " 'opponent',\n",
       " 'siblings',\n",
       " 'unusual',\n",
       " 'requests',\n",
       " 'boat',\n",
       " 'rick',\n",
       " 'mansion',\n",
       " 'director',\n",
       " 'salvatore',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for item in (datas[\"cleaned\"]):\n",
    "    x= item.split()\n",
    "    counter.update(x)\n",
    "# print(len(list(counter)))\n",
    "# print(list(counter.most_common(1000)))\n",
    "# print(len(list(counter)))\n",
    "most_commons = list(map(lambda x : x[0] ,counter.most_common(3000)))\n",
    "most_commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Plots and creating tfidf matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>life</th>\n",
       "      <th>time</th>\n",
       "      <th>world</th>\n",
       "      <th>-</th>\n",
       "      <th>family</th>\n",
       "      <th>story</th>\n",
       "      <th>war</th>\n",
       "      <th>love</th>\n",
       "      <th>named</th>\n",
       "      <th>own</th>\n",
       "      <th>...</th>\n",
       "      <th>priorities</th>\n",
       "      <th>avenues</th>\n",
       "      <th>fletcher—huggo</th>\n",
       "      <th>1800s</th>\n",
       "      <th>diary</th>\n",
       "      <th>containing</th>\n",
       "      <th>magic</th>\n",
       "      <th>diaries</th>\n",
       "      <th>trick</th>\n",
       "      <th>it—gary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.665546</td>\n",
       "      <td>1.707744</td>\n",
       "      <td>1.744727</td>\n",
       "      <td>1.935542</td>\n",
       "      <td>1.920819</td>\n",
       "      <td>1.853872</td>\n",
       "      <td>1.950782</td>\n",
       "      <td>1.935542</td>\n",
       "      <td>1.950782</td>\n",
       "      <td>1.966576</td>\n",
       "      <td>...</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "      <td>3.39794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142283</td>\n",
       "      <td>0.068662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.030283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031722</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.087310</td>\n",
       "      <td>0.016853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         life      time     world         -    family     story       war  \\\n",
       "0    1.665546  1.707744  1.744727  1.935542  1.920819  1.853872  1.950782   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.106712  0.000000  0.036126   \n",
       "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.142283  0.068662  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "247  0.030283  0.000000  0.031722  0.017596  0.087310  0.016853  0.000000   \n",
       "248  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "249  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.042408   \n",
       "250  0.000000  0.037125  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         love     named       own  ...  priorities  avenues  fletcher—huggo  \\\n",
       "0    1.935542  1.950782  1.966576  ...     3.39794  3.39794         3.39794   \n",
       "1    0.000000  0.067268  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "2    0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "3    0.037952  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "4    0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "..        ...       ...       ...  ...         ...      ...             ...   \n",
       "246  0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "247  0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "248  0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "249  0.000000  0.000000  0.000000  ...     0.00000  0.00000         0.00000   \n",
       "250  0.000000  0.000000  0.042752  ...     0.00000  0.00000         0.00000   \n",
       "\n",
       "       1800s    diary  containing    magic  diaries    trick  it—gary  \n",
       "0    3.39794  3.39794     3.39794  3.39794  3.39794  3.39794  3.39794  \n",
       "1    0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "2    0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "3    0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "4    0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "..       ...      ...         ...      ...      ...      ...      ...  \n",
       "246  0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "247  0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "248  0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "249  0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "250  0.00000  0.00000     0.00000  0.00000  0.00000  0.00000  0.00000  \n",
       "\n",
       "[251 rows x 3000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_frame = pd.DataFrame(columns=  most_commons)\n",
    "results = []\n",
    "for item in most_commons:\n",
    "    c = 0\n",
    "    for val in datas[\"cleaned\"]:\n",
    "        if item in val.split():\n",
    "            c+=1\n",
    "    # print(c)\n",
    "    # print(math.log(250/c , 10) +1)\n",
    "    results.append(math.log(250/c , 10) +1)\n",
    "# print(max(results))\n",
    "new_data_frame.loc[0] =  results   \n",
    "new_data_frame\n",
    "hint = 0\n",
    "# print(datas.loc[0])\n",
    "for i in  range(0,250):\n",
    "    l = {}\n",
    "    res = datas[\"cleaned\"].loc[i].split()\n",
    "    # print(res)\n",
    "    for j in range(3000):\n",
    "        item = most_commons[j]\n",
    "        # print(item)\n",
    "        # if i == 77 and j == 406:\n",
    "            # print(res.count(item)/len(res)*new_data_frame.loc[0][j])\n",
    "        # print(res)\n",
    "        l[item] = (res.count(item)/len(res))*new_data_frame.loc[0][j]\n",
    "    # if i == 77 and j == 406:\n",
    "    \n",
    "    # print(set(l))\n",
    "   \n",
    "    \n",
    "    new_data_frame = pd.concat([new_data_frame , pd.DataFrame(l , index = [i+1])] )\n",
    "    # new_data_frame = pd.concat([ pd.DataFrame(l , index = [i+1])  , new_data_frame  ]   )\n",
    "    \n",
    "    # new_data_frame = new_data_frame.append(l , ignore_index = True)\n",
    "# new_data_frame = new_data_frame[:-1]\n",
    "new_data_frame\n",
    "    # new_data_frame.loc[i] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similar Movies to enterd plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9696506216402684, 1), (0.3801020240877696, 3), (0.10458962953305755, 195), (0.08715093603963064, 246), (0.08441158309112674, 35)]\n",
      "[' The Godfather', ' The Godfather Part II', ' The Deer Hunter', ' Drishyam', ' Gladiator']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def finding_similiars(new_data_frame,plot):\n",
    "    plot = plot.replace(\".\" , \"\").replace(\"!\" , \"\").replace(\"?\" , \"\").replace(\",\" , \"\").replace(\"'\" , '').replace(\"'\" , \"\")\n",
    "    plot = plot.split()\n",
    "    plot = list(map(lambda v: v.lower() , plot ))\n",
    "    # print(plot)\n",
    "    # for u in x:\n",
    "    #     print(u)\n",
    "    #     if u in res  + [\"a\"]:\n",
    "    #         # print(\"it did\")\n",
    "    #         x.remove(u)\n",
    "    # print(x)\n",
    "    plot = list(filter(lambda x: x  not in stop_words+[\"a\"] , plot))\n",
    "    # print(plot)\n",
    "    item = \" \".join(plot)\n",
    "    # print(plot)\n",
    "    l = {}\n",
    "    for i in range(3000):\n",
    "        # print(new_data_frame.loc[0][i])\n",
    "        # print(new_data_frame.loc[0][i])\n",
    "        # print(plot.count(most_commons[i]))\n",
    "        # print(((plot.count(most_commons[i])/len(plot))*new_data_frame.loc[0][i]))\n",
    "        l[most_commons[i]] = (plot.count(most_commons[i])/len(plot))*new_data_frame.loc[0][i]\n",
    "    # print(l)\n",
    "    x = pd.DataFrame(l , index = [251])\n",
    "    mag_x = norm(x.loc[251])\n",
    "    \n",
    "    new_data_frame = pd.concat([new_data_frame , x] , ignore_index = True)\n",
    "    # print(new_data_frame.tail())\n",
    "    \n",
    "    # print(list(set(x.loc[])))\n",
    "    # print(list(x.loc[251]))\n",
    "        # for x in l.values:\n",
    "    # print(max(l.values))        \n",
    "    # print(x)\n",
    "    # # print(new_data_frame.head())\n",
    "    # print(np.array(new_data_frame.loc[251]))\n",
    "    # print(\"\\n\")\n",
    "    # print(np.array(new_data_frame.loc[2]))\n",
    "        #pd.DataFrame(l , index = [251]))\n",
    "    # new_data_frame = pd.concat([new_data_frame , pd.DataFrame(l , index = [251])] , ignore_index = True)\n",
    "    # print(max(new_data_frame.loc[251]))\n",
    "    # print(new_data_frame.loc[251])\n",
    "    # print(datas.loc[249])\n",
    "    final_results = list()\n",
    "    for i in range(250):\n",
    "        mag_y = norm(np.array(new_data_frame.loc[i+1]))\n",
    "        # if i==249 : \n",
    "        #     print((np.inner(np.array(new_data_frame.loc[i+1]), np.array(new_data_frame.loc[250]).reshape(3000 , ))/(mag_x*mag_y)))\n",
    "            # print(new_data_frame.loc[i+1])\n",
    "            # print(new_data_frame.loc[250])\n",
    "        # print((new_data_frame.loc[i][0]) , (x.loc[251][0]))\n",
    "        # print(new_data_frame.loc[i])\n",
    "        # print((np.inner(new_data_frame.loc[i] , x.loc[251]) , i))\n",
    "        final_results.append((np.inner(np.array(new_data_frame.loc[i+1]), np.array(new_data_frame.loc[251]).reshape(3000 , ))/(mag_x*mag_y) , i))\n",
    "        \n",
    "    seven_best_ones = sorted(final_results , key= lambda x : x[0] , reverse =True )[:5]\n",
    "    print(seven_best_ones)\n",
    "    best_film_names = [datas[\"Name\"].loc[y] for x,y in seven_best_ones]\n",
    "    \n",
    "    \n",
    "    # item = item.replace(\".\" , \"\").replace(\"!\" , \"\").replace(\"?\" , \"\").replace(\",\" , \"\").replace(\"'\" , '').replace(\"'\" , \"\")\n",
    "    # x= item.split()\n",
    "    \n",
    "    # x = list(map(lambda v: v.lower() , x ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(x  for x in new_data_frame.loc[173])\n",
    "    # print(item)\n",
    "    \n",
    "    \n",
    "    # for u in (datas.loc[195][\"cleaned\"].split()):\n",
    "    #     # print(u)s\n",
    "        \n",
    "    #     if u in item.split() :\n",
    "    #         print(u)\n",
    "    # print(item)\n",
    "    # for key in l.keys():\n",
    "    #     if l[key]!= 0:\n",
    "    #         print(key)\n",
    "    # print(x.loc[251])\n",
    "    # print(new_data_frame.loc[1])\n",
    "    # print((list(x.loc[251])))\n",
    "    # for i in range(1000) :\n",
    "    #     if list(x.loc[251])[i]!= 0 :\n",
    "    #         print(i)\n",
    "    # print(\"\\n\")\n",
    "    # # # print((list(new_data_frame.loc[1])))\n",
    "    # for i in range(1000) :\n",
    "    #     if list(new_data_frame.loc[77])[i]!= 0 :\n",
    "    #         print(most_commons[i])\n",
    "            \n",
    "    # for p in item.split():\n",
    "    #     print(p)\n",
    "    # print(\"\\n\")\n",
    "    # for item in new_data_frame.loc[57].split(:\n",
    "    #     print(item)\n",
    "        # if p in datas.loc[57][\"cleaned\"].split():\n",
    "    # print(datas.loc[230][\"cleaned\"])\n",
    "    # print(list(new_data_frame.loc[57]) )    \n",
    "    print(best_film_names)\n",
    "\n",
    "    # print(new_data_frame)\n",
    "    # return new_data_frame\n",
    "# stop_words\n",
    "finding_similiars(new_data_frame , \"The Godfather 'Don' Vito Corleone is the head of the Corleone mafia family in New York. He is at the event of his daughter's wedding. Michael, Vito's youngest son and a decorated WWII Marine is also present at the wedding. Michael seems to be uninterested in being a part of the family business. Vito is a powerful man, and is kind to all those who give him respect but is ruthless against those who do not. But when a powerful and treacherous rival wants to sell drugs and needs the Don's influence for the same, Vito refuses to do it. What follows is a clash between Vito's fading old values and the new ways which may cause Michael to do the thing he was most reluctant in doing and wage a mob war against all the other mafia families which could tear the Corleone family apart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ma-\n",
      "[nltk_data]     na\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ma-\n",
      "[nltk_data]     na\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ma-\n",
      "[nltk_data]     na\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\ma-\n",
      "[nltk_data]     na\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      chronicles experiences formerly successful ban...\n",
       "1      godfather \"don\" vito corleone head corleone ma...\n",
       "2      set events batman begins (2005) batman lieuten...\n",
       "3      continuing saga corleone crime family tells st...\n",
       "4      defense prosecution rested jury filing jury de...\n",
       "                             ...                        \n",
       "245    ellie andrews tied knot society aviator king w...\n",
       "246    vijay salgaonkar runs cable tv network remote ...\n",
       "247    shahid khan exiled impersonating legendary sul...\n",
       "248    lt john dunbar dubbed hero accidentally leads ...\n",
       "249    aladdin poor street urchin spends time stealin...\n",
       "Name: cleaned, Length: 250, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "nltk_res  =list()\n",
    "for word in datas[\"Plot\"]:\n",
    "    # print(str(word))\n",
    "# example_sent = \"\"\"This is a sample sentence,\n",
    "#                   showing off the stop words filtration.\"\"\"\n",
    " \n",
    "    \n",
    "    word_tokens = word_tokenize(str(word))\n",
    "    # converts the words in word_tokens to lower case and then checks whether \n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    #with no lower case conversion\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    nltk_res.append(\" \".join(filtered_sentence))\n",
    "    # print(word_tokens)\n",
    "    # print(filtered_sentence)\n",
    "datas[\"nltk_cleaned\"] = nltk_res\n",
    "datas[\"cleaned\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vectorizer and tfidf matrice and creating knn model and fitting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1 , stop_words=\"english\")\n",
    "tfidf = vectorizer.fit_transform(datas[\"nltk_cleaned\"]).toarray()\n",
    "# print(tfidf[0])\n",
    "model_knn = NearestNeighbors(n_neighbors=5,metric = 'cosine', algorithm = 'brute')\n",
    "model_knn.fit(tfidf)\n",
    "# tfidf\n",
    "# print(vectorizer.fit_transform(datas[\"nltk_cleaned\"]))\n",
    "# for item in list(datas[\"nltk_cleaned\"]):\n",
    "#     print(vectorizer.fit_transform(item))\n",
    "# X = vectorizer.fit_transform(list(datas[\"nltk_cleaned\"]))\n",
    " # Inspect feature names and TF-IDF values \n",
    "# print(vectorizer.get_feature_names_out()) \n",
    "# # Convert the variable to an array \n",
    "# X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding similar films to entered plot using knn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Godfather\n",
      " The Godfather Part II\n",
      " Trainspotting\n",
      " Singin' in the Rain\n",
      " Goodfellas\n"
     ]
    }
   ],
   "source": [
    "# new_data_frame\n",
    "# def dot(v1, v2):\n",
    "#     return sum(x*y for , y in zip(v1, v2))\n",
    "# print(datas.loc[169])\n",
    "def finding_similiarsـnew(new_data_frame,plot_n , model):\n",
    "    # input=[\"input\"]\n",
    "    plot_n= [plot_n]\n",
    "    plot_n = vectorizer.transform(plot_n).toarray()\n",
    "    # print(plot_n)\n",
    "    suggestions = model.kneighbors(plot_n , 5)\n",
    "    # model.fit(datas[\"nltk_cleaned\"])\n",
    "    # print(model.kneighbors([plot], 2, return_distance=False))\n",
    "\n",
    "    # plot = plot.replace(\".\" , \"\").replace(\"!\" , \"\").replace(\"?\" , \"\").replace(\",\" , \"\").replace(\"'\" , '').replace(\"'\" , \"\")\n",
    "    # plot = plot.split()\n",
    "    # plot = list(map(lambda v: v.lower() , plot ))\n",
    "    # print(plot)\n",
    "    # for u in x:\n",
    "    #     print(u)\n",
    "    #     if u in res  + [\"a\"]:\n",
    "    #         # print(\"it did\")\n",
    "    #         x.remove(u)\n",
    "    # print(x)\n",
    "    # plot = list(filter(lambda x: x  not in stop_words+[\"a\"] , plot))\n",
    "    # # print(plot)\n",
    "    # item = \" \".join(plot)\n",
    "    # # print(plot)\n",
    "    # l = {}\n",
    "    # for i in range(3000):\n",
    "    #     # print(new_data_frame.loc[0][i])\n",
    "    #     # print(new_data_frame.loc[0][i])\n",
    "    #     # print(plot.count(most_commons[i]))\n",
    "    #     # print(((plot.count(most_commons[i])/len(plot))*new_data_frame.loc[0][i]))\n",
    "    #     l[most_commons[i]] = (plot.count(most_commons[i])/len(plot))*new_data_frame.loc[0][i]\n",
    "    # print(l)\n",
    "    # x = pd.DataFrame(l , index = [251])\n",
    "    # mag_x = norm(plot_n)\n",
    "    for suggestion in suggestions[1][0]:\n",
    "        # print(suggestion)\n",
    "        print(datas.loc[suggestion][\"Name\"])\n",
    "    # # new_data_frame = pd.concat([new_data_frame , x] , ignore_index = True)\n",
    "    # # print(new_data_frame.tail())\n",
    "    \n",
    "    # # print(list(set(x.loc[])))\n",
    "    # # print(list(x.loc[251]))\n",
    "    #     # for x in l.values:\n",
    "    # # print(max(l.values))        \n",
    "    # # print(x)\n",
    "    # # # print(new_data_frame.head())\n",
    "    # # print(np.array(new_data_frame.loc[251]))\n",
    "    # # print(\"\\n\")\n",
    "    # # print(np.array(new_data_frame.loc[2]))\n",
    "    #     #pd.DataFrame(l , index = [251]))\n",
    "    # # new_data_frame = pd.concat([new_data_frame , pd.DataFrame(l , index = [251])] , ignore_index = True)\n",
    "    # # print(max(new_data_frame.loc[251]))\n",
    "    # # print(new_data_frame.loc[251])\n",
    "    # # print(datas.loc[249])\n",
    "    # final_results = list()\n",
    "    # for i in range(250):\n",
    "    #     mag_y = norm(tfidf[i])\n",
    "    #     # if i==249 : \n",
    "    #     #     print((np.inner(np.array(new_data_frame.loc[i+1]), np.array(new_data_frame.loc[250]).reshape(3000 , ))/(mag_x*mag_y)))\n",
    "    #         # print(new_data_frame.loc[i+1])\n",
    "    #         # print(new_data_frame.loc[250])\n",
    "    #     # print((new_data_frame.loc[i][0]) , (x.loc[251][0]))\n",
    "    #     # print(new_data_frame.loc[i])\n",
    "    #     # print((np.inner(new_data_frame.loc[i] , x.loc[251]) , i))\n",
    "    #     final_results.append((np.inner(tfidf[i] , plot_n)/(mag_y*mag_x) , i))\n",
    "        \n",
    "    # seven_best_ones = sorted(final_results , key= lambda x : x[0] , reverse =True )[:7]\n",
    "    # print(suggestions[1])\n",
    "    # best_film_names = [datas[\"Name\"].loc[y] for x,y in seven_best_ones]\n",
    "    \n",
    "    \n",
    "    # item = item.replace(\".\" , \"\").replace(\"!\" , \"\").replace(\"?\" , \"\").replace(\",\" , \"\").replace(\"'\" , '').replace(\"'\" , \"\")\n",
    "    # x= item.split()\n",
    "    \n",
    "    # x = list(map(lambda v: v.lower() , x ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(x  for x in new_data_frame.loc[173])\n",
    "    # print(item)\n",
    "    \n",
    "    \n",
    "    # for u in (datas.loc[195][\"cleaned\"].split()):\n",
    "    #     # print(u)s\n",
    "        \n",
    "    #     if u in item.split() :\n",
    "    #         print(u)\n",
    "    # print(item)\n",
    "    # for key in l.keys():\n",
    "    #     if l[key]!= 0:\n",
    "    #         print(key)\n",
    "    # print(x.loc[251])\n",
    "    # print(new_data_frame.loc[1])\n",
    "    # print((list(x.loc[251])))\n",
    "    # for i in range(1000) :\n",
    "    #     if list(x.loc[251])[i]!= 0 :\n",
    "    #         print(i)\n",
    "    # print(\"\\n\")\n",
    "    # # # print((list(new_data_frame.loc[1])))\n",
    "    # for i in range(1000) :\n",
    "    #     if list(new_data_frame.loc[77])[i]!= 0 :\n",
    "    #         print(most_commons[i])\n",
    "            \n",
    "    # for p in item.split():\n",
    "    #     print(p)\n",
    "    # print(\"\\n\")\n",
    "    # for item in new_data_frame.loc[57].split(:\n",
    "    #     print(item)\n",
    "        # if p in datas.loc[57][\"cleaned\"].split():\n",
    "    # print(datas.loc[230][\"cleaned\"])\n",
    "    # print(list(new_data_frame.loc[57]) )    \n",
    "    # print(best_film_names)\n",
    "\n",
    "    # print(new_data_frame)\n",
    "    # return new_data_frame\n",
    "    \n",
    "# stop_words\n",
    "finding_similiarsـnew(new_data_frame , \"The Godfather 'Don' Vito Corleone is the head of the Corleone mafia family in New York. He is at the event of his daughter's wedding. Michael, Vito's youngest son and a decorated WWII Marine is also present at the wedding. Michael seems to be uninterested in being a part of the family business. Vito is a powerful man, and is kind to all those who give him respect but is ruthless against those who do not. But when a powerful and treacherous rival wants to sell drugs and needs the Don's influence for the same, Vito refuses to do it. What follows is a clash between Vito's fading old values and the new ways which may cause Michael to do the thing he was most reluctant in doing and wage a mob war against all the other mafia families which could tear the Corleone family apart.\" ,model_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
